/**
 * Chunk Inspection Script
 * Processes sample documents and saves chunks to JSON for inspection
 * Run with: node scripts/inspect-chunks.js
 */

const path = require('path');
const fs = require('fs');
const Module = require('module');

// Mock vscode module by intercepting require
const originalRequire = Module.prototype.require;
Module.prototype.require = function(id) {
  if (id === 'vscode') {
    return {
      workspace: {
        getConfiguration: () => ({
          get: (key, defaultValue) => {
            if (key === 'embeddingModel') return 'Xenova/all-MiniLM-L6-v2';
            if (key === 'pdfStructureDetection') return 'heuristic';
            return defaultValue;
          }
        })
      },
      window: {
        withProgress: async (options, task) => {
          const progress = {
            report: (value) => console.log(`Progress: ${value.message || ''}`)
          };
          return await task(progress);
        }
      },
      ProgressLocation: {
        Notification: 15,
        Window: 10,
        SourceControl: 1
      }
    };
  }
  return originalRequire.apply(this, arguments);
};

// Now load our compiled modules
const { DocumentProcessor } = require('../out/src/documentProcessor');
const { EmbeddingService } = require('../out/src/embeddingService');

async function inspectDocument(filePath, chunkSize = 512, overlap = 50) {
  console.log(`\nüìÑ Processing: ${path.basename(filePath)}`);
  console.log('‚îÅ'.repeat(60));

  const embeddingService = EmbeddingService.getInstance();

  // Process document
  console.log('  Step 1: Reading and processing document...');
  const doc = await DocumentProcessor.processDocument(filePath);

  // Create semantic chunks
  console.log('  Step 2: Creating semantic chunks...');
  const chunks = DocumentProcessor.splitIntoSemanticChunks(doc.markdown, chunkSize, overlap);
  console.log(`  ‚úì Created ${chunks.length} chunks`);

  // Generate embeddings
  console.log('  Step 3: Generating embeddings...');
  const texts = chunks.map(c => c.text);
  const embeddings = await embeddingService.embedBatch(texts);
  console.log(`  ‚úì Generated ${embeddings.length} embeddings`);

  // Format output
  const output = {
    documentName: doc.metadata.fileName,
    filePath: filePath,
    fileType: doc.metadata.fileType,
    processedAt: new Date().toISOString(),
    metadata: {
      totalCharacters: doc.text.length,
      totalChunks: chunks.length,
      chunkSize: chunkSize,
      overlap: overlap,
      embeddingModel: embeddingService.getCurrentModel() || 'unknown',
      embeddingDimension: embeddings[0]?.length || 0,
    },
    chunks: chunks.map((chunk, idx) => ({
      index: idx,
      sectionTitle: chunk.sectionTitle,
      headingPath: chunk.headingPath,
      headingLevel: chunk.headingLevel,
      text: chunk.text,
      textLength: chunk.text.length,
      position: {
        start: chunk.startPosition,
        end: chunk.endPosition,
      }
    })),
  };

  console.log('  ‚úì Document processing complete!');
  return output;
}

async function main() {
  console.log('\nüîç RAG Chunk Inspector');
  console.log('‚ïê'.repeat(60));

  const fixturesPath = path.join(__dirname, '../test/fixtures');
  const outputDir = path.join(__dirname, '../test/chunk-output');

  // Create output directory
  if (!fs.existsSync(outputDir)) {
    fs.mkdirSync(outputDir, { recursive: true });
    console.log(`\nüìÅ Created output directory: ${outputDir}`);
  }

  // Initialize embedding service
  console.log('\n=== Initializing Embedding Model ===');
  const EmbeddingService = require('../out/src/embeddingService').EmbeddingService;
  const embeddingService = EmbeddingService.getInstance();
  await embeddingService.initialize('Xenova/all-MiniLM-L6-v2');
  console.log('Model initialized successfully!\n');

  // Documents to process
  const documents = [
    { name: 'sample.md', path: path.join(fixturesPath, 'sample.md') },
    { name: 'sample.html', path: path.join(fixturesPath, 'sample.html') },
    { name: 'sample-text.txt', path: path.join(fixturesPath, 'sample-text.txt') },
  ];

  const allResults = [];

  // Process each document
  for (const doc of documents) {
    if (fs.existsSync(doc.path)) {
      try {
        const result = await inspectDocument(doc.path);
        allResults.push(result);

        // Save individual document output
        const outputPath = path.join(outputDir, `${doc.name}.json`);
        fs.writeFileSync(outputPath, JSON.stringify(result, null, 2));
        console.log(`  üíæ Saved to: ${outputPath}\n`);
      } catch (error) {
        console.error(`  ‚ùå Error processing ${doc.name}:`, error.message);
      }
    } else {
      console.log(`  ‚ö†Ô∏è  File not found: ${doc.path}\n`);
    }
  }

  // Save combined output
  const combinedPath = path.join(outputDir, 'all-chunks.json');
  fs.writeFileSync(combinedPath, JSON.stringify(allResults, null, 2));
  console.log(`\nüíæ Combined output saved to: ${combinedPath}`);

  // Print summary
  console.log('\nüìä Summary');
  console.log('‚ïê'.repeat(60));
  allResults.forEach((result) => {
    console.log(`\n${result.documentName}:`);
    console.log(`  ‚Ä¢ Total chunks: ${result.metadata.totalChunks}`);
    console.log(`  ‚Ä¢ Total characters: ${result.metadata.totalCharacters}`);
    console.log(`  ‚Ä¢ Embedding dimension: ${result.metadata.embeddingDimension}`);
    console.log(`  ‚Ä¢ Sections:`);

    const sections = new Set(result.chunks.map(c => c.sectionTitle));
    sections.forEach(section => {
      const count = result.chunks.filter(c => c.sectionTitle === section).length;
      console.log(`    - ${section}: ${count} chunk(s)`);
    });
  });

  // Print sample chunks
  console.log('\n\nüìù Sample Chunks (First chunk from each document)');
  console.log('‚ïê'.repeat(60));
  allResults.forEach((result) => {
    if (result.chunks.length > 0) {
      const chunk = result.chunks[0];
      console.log(`\n[${result.documentName}] Chunk #${chunk.index}`);
      console.log(`Section: ${chunk.sectionTitle}`);
      console.log(`Path: ${chunk.headingPath.join(' ‚Üí ')}`);
      console.log(`Text (${chunk.textLength} chars): "${chunk.text.substring(0, 150)}..."`);
    }
  });

  console.log('\n\n‚úÖ Chunk inspection complete!');
  console.log(`\nüìÇ All files saved in: ${outputDir}`);
  console.log('\nYou can now inspect the JSON files to see:');
  console.log('  ‚Ä¢ Chunk text and structure');
  console.log('  ‚Ä¢ Section hierarchy');
  console.log('  ‚Ä¢ Full embedding vectors');
  console.log('  ‚Ä¢ Position information');
}

// Run the script
main().catch(err => {
  console.error('\n‚ùå Error:', err);
  process.exit(1);
});

